{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "126QA7aPNwXFX5tHtTnNZfZzTJCNu4uYF",
      "authorship_tag": "ABX9TyNFVKkYETRTwtMpgZEVa+RB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudev49/bnn/blob/main/ImageCNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrFQ1CyGgQI6"
      },
      "source": [
        "%%capture\n",
        "!pip install ipdb\n",
        "!pip install pyro-ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzS7mS7Egao7",
        "outputId": "3d3400e5-7db3-403e-a1c0-63eae7893430"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/BNN/')\n",
        "\n",
        "\n",
        "# !unzip 'test-20210504T120542Z-001.zip'\n",
        "# !unzip 'Containers-20210504T120540Z-001.zip'\n",
        "\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Al-Sibahi_MNIST_1.ipynb\t       HPOResults.gsheet\n",
            " BNN-FromKaggle.ipynb\t\t       ImageCNN_MNIST.ipynb\n",
            " BNN_Linear_Regression_Example.ipynb   ImageCNN_RUST.ipynb\n",
            " cifar-10-batches-py-data\t       ListOfHPOParams.csv\n",
            " Containers\t\t\t       ListOfHPOParams.gsheet\n",
            " Containers-20210504T120540Z-001.zip   MNIST\n",
            "'Copy of bnn_parasChopra.ipynb'        mnist-data\n",
            "'Copy of HPOResults.csv'\t       Pyro_intro.ipynb\n",
            "'Copy of HPOResults.gsheet'\t       Pyro_Poutine.ipynb\n",
            " data\t\t\t\t       test\n",
            " HPOResults.csv\t\t\t       test-20210504T120542Z-001.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5khFJIpPgasD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import functools\n",
        "import operator\n",
        "import ipdb\n",
        "import pyro\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime, timezone\n",
        "import pytz\n",
        "\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "pyro.set_rng_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6U_Lk5HgavX"
      },
      "source": [
        "# config:\n",
        "\n",
        "img_height = 28\n",
        "img_width = 28\n",
        "input_dim = [1,img_height, img_width]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
        "                                  else \"cpu\")\n",
        "n_epochs = 1\n",
        "device = torch.device('cpu')\n",
        "batch_size = 500\n",
        "n_train_data=20000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT-YCmT_gayT",
        "outputId": "5c625efd-49e7-43ec-b1bf-811f4a3e8d8c"
      },
      "source": [
        "# def load_transform_split(trainDataDir, testDataDir, img_height = 180, img_width = 180, valid_size=.2, batch_size  = 5):\n",
        "#   # ipdb.set_trace()\n",
        "#   train_transform = transforms.Compose([transforms.ToTensor(),])\n",
        "#   test_transform = transforms.Compose([transforms.ToTensor(),])\n",
        "#   trainData= datasets.ImageFolder(trainDataDir,transform = train_transform)\n",
        "#   testData = datasets.ImageFolder(testDataDir, transform = test_transform)  \n",
        "#   trainLoader = torch.utils.data.DataLoader(trainData, batch_size= batch_size , shuffle=True)\n",
        "#   testLoader = torch.utils.data.DataLoader(testData, batch_size= batch_size , shuffle=True)\n",
        "#   return trainLoader, testLoader, trainData.class_to_idx, testData.class_to_idx\n",
        "# trainLoader, testLoader, train_class_to_idx, test_class_to_idx =  load_transform_split(\"Containers\", \"test\")\n",
        "# # print(next(iter(trainLoader)))\n",
        "trainData_ = datasets.MNIST('mnist-data', train=True, download=True,transform = transforms.Compose([transforms.ToTensor(),]))\n",
        "len_trainData_ = trainData_.data.shape[0]\n",
        "trainData, valData = torch.utils.data.random_split(trainData_, [n_train_data,len_trainData_-n_train_data])\n",
        "\n",
        "testData = datasets.MNIST('mnist-data', train=False, download=True,transform = transforms.Compose([transforms.ToTensor(),] ))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainData, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(testData, batch_size=batch_size)\n",
        "\n",
        " \n",
        "print(trainData.dataset.class_to_idx)\n",
        "print(testData.class_to_idx)\n",
        "\n",
        "len(trainData)\n",
        "# trainDataSubset = torch.utils.data.RandomSampler(data_source=trainData, replacement=True, num_samples=20000, generator=None)\n",
        "\n",
        "# for image, labels in trainDataSubset:\n",
        "#   print(image.shape)\n",
        "#   print(label.shape)\n",
        "#  np.random.randint(0,10,5)\n",
        "\n",
        "# np.random.seed(0)\n",
        "# np.random.choice(range(10),5, replace=False)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0 - zero': 0, '1 - one': 1, '2 - two': 2, '3 - three': 3, '4 - four': 4, '5 - five': 5, '6 - six': 6, '7 - seven': 7, '8 - eight': 8, '9 - nine': 9}\n",
            "{'0 - zero': 0, '1 - one': 1, '2 - two': 2, '3 - three': 3, '4 - four': 4, '5 - five': 5, '6 - six': 6, '7 - seven': 7, '8 - eight': 8, '9 - nine': 9}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Ha1te9ga08"
      },
      "source": [
        "from torch.nn import NLLLoss \n",
        "class PlainCNN(nn.Module):\n",
        "  def __init__(self, in_channels=1, out_channels=16, outdim=10, input_dim=input_dim):\n",
        "    super(PlainCNN, self).__init__()\n",
        "    # self.cuda()\n",
        "    self.CNN_layer = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True), \n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(in_channels=out_channels, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(4),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Flatten()\n",
        "        )\n",
        "    num_features_from_CNN_layer = functools.reduce(operator.mul, list(self.CNN_layer(torch.rand(1, *input_dim)).shape))\n",
        "    self.Linear_layer = nn.Sequential(\n",
        "            nn.Linear(num_features_from_CNN_layer, outdim),\n",
        "            # nn.LogSoftmax()\n",
        "            )\n",
        "  def forward(self, x):\n",
        "      x = self.CNN_layer(x)\n",
        "      x = self.Linear_layer(x)  ## note: outputs logits... not softmax.\n",
        "      return x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqV9V6EIFwbb"
      },
      "source": [
        "\n",
        "# model = PlainCNN()\n",
        "\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "# # lossfn=NLLLoss()\n",
        "# lossfn = nn.CrossEntropyLoss() ## CrossEntropyLoss expects logits as inputs\n",
        "\n",
        "# model = model.to(device)\n",
        "# lossfn = lossfn.to(device)\n",
        "\n",
        "# for i in range(n_epochs):\n",
        "#   running_loss = 0\n",
        "#   batch_no = 0\n",
        "#   for images, labels in train_loader:\n",
        "#     batch_no+=1\n",
        "#     images = images.to(device)\n",
        "#     labels = labels.to(device)\n",
        "#     optimizer.zero_grad()\n",
        "#     # ipdb.set_trace()\n",
        "#     outputs= model(images)\n",
        "#     loss = lossfn(outputs.squeeze(), labels)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     running_loss += loss.item()\n",
        "#     # break;\n",
        "#     if batch_no%50==0:\n",
        "#         print (f'epoch: {i} Batch: {batch_no} loss: {running_loss}')\n",
        "\n",
        "#how many params are there?\n",
        "# sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# for images, labels in trainLoader:\n",
        "#   print(model(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h12mgGWrgjUM"
      },
      "source": [
        "# resultList =[]\n",
        "# model_perf=0\n",
        "# for image, labels in test_loader:\n",
        "#   # ipdb.set_trace()\n",
        "#   result = np.argmax(model(image.to(device)).detach().numpy(), axis=1)\n",
        "#   resultList.extend(result.tolist())\n",
        "#   model_perf += sum(labels.numpy() == result)\n",
        "\n",
        "# resultList\n",
        "# model_perf/len(test_loader.dataset)\n",
        "\n",
        "\n",
        "# i=0\n",
        "# bs = test_loader.batch_size\n",
        "# for image, labels in test_loader:\n",
        "#   for img in image:\n",
        "#     # plt.imshow(img.permute(1,2,0).squeeze())\n",
        "#     # plt.show()\n",
        "#     print(f'Actual Label:{labels[i%bs]}, Predicted:{np.argmax(np.exp(resultList[i]))}')\n",
        "#     i+=1\n",
        "#   if i>10:\n",
        "#     break\n",
        "#     print(\"------------\")\n",
        "\n",
        "# len(test_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIbLQkWagxOC"
      },
      "source": [
        "## Bayesi-*fy* the CNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlRrHdzpgjXf"
      },
      "source": [
        "from pyro.nn.module import to_pyro_module_\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.nn import PyroModule, PyroSample\n",
        "import torch.nn as nn\n",
        "# from pyro.infer.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
        "from pyro.infer import autoguide\n",
        "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
        "from tqdm.auto import trange, tqdm\n",
        "from pyro.optim import Adam as pyro_ka_Adam\n",
        "\n",
        "\n",
        "# def __init__(self, in_channels=3, out_channels=16, input_dim = [3,img_height, img_width]):\n",
        "\n",
        "class BayesianCNN(PyroModule):\n",
        "  def __init__(self, CNN_with_priors):\n",
        "    super().__init__()\n",
        "    self.neural_network = CNN_with_priors\n",
        "  def forward(self, X_train, y_train=None):\n",
        "    # sigma = pyro.sample(\"sigma\", dist.Uniform(0.,10.))\n",
        "    mean = self.neural_network(X_train).squeeze()\n",
        "    with pyro.plate(\"data\", X_train.shape[0], device = device):\n",
        "      # obs = pyro.sample(\"obs\", dist.Categorical(logits = mean), obs=y_train)\n",
        "      # obs =  pyro.sample(\"obs\", dist.Categorical(logits = mean), obs=y_train)\n",
        "      pyro.sample(\"obs\", dist.Categorical(logits = mean), obs=y_train)\n",
        "\n",
        "    return (mean)  \n",
        "    # we will return this mean and while predicting these will be simulated too using the _RETURN return_sites in Predictive()\n",
        "    # this mean is the logits of the classifier prediction of the 10 numbers\n",
        "\n",
        "# dist.Categorical(logits = torch.tensor([1.,2.,3.]))\n",
        "# pyro.sample(\"obs\", dist.Categorical(logits = torch.tensor([1.,2.,3.])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq12yAEwDUXm"
      },
      "source": [
        "# guide = autoguide.AutoDiagonalNormal(bcnn, init_scale=inital_variance)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTe4tR6uIQBW"
      },
      "source": [
        "# bnn config\n",
        "\n",
        "inital_variance = 0.01\n",
        "n_epochs = 14\n",
        "learning_rate = 0.001\n",
        "n_samples=200\n",
        "early_stopping_tol=0.005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPEi6lVpga4A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "42fc954a-014d-4d69-c775-e783dcb770b4"
      },
      "source": [
        "# from importlib import reload  \n",
        "# import torch.optim as optim\n",
        "# reload( torch.optim)\n",
        "from torch.distributions import constraints\n",
        "\n",
        "\n",
        "PlainCNNModel = PlainCNN()\n",
        "PlainCNNModel = PlainCNNModel.to(device)\n",
        "to_pyro_module_(PlainCNNModel)\n",
        "for m in PlainCNNModel.modules():\n",
        "    # print(f'--------------- module:{m} -----------------')\n",
        "    for name, value in list(m.named_parameters(recurse=False)):\n",
        "      # print(f'************* name: {name} *************')    \n",
        "      # print(f'>>>>>> weight.shape: {value.shape}<<<<<<<<<')  \n",
        "      setattr(m, name, PyroSample(prior=dist.Normal(0, 1)\n",
        "                                        .expand(value.shape)\n",
        "                                        .to_event(value.dim())\n",
        "                                        ))\n",
        "# PlainCNNModel = PlainCNNModel.to(device)\n",
        "bcnn = BayesianCNN(PlainCNNModel)\n",
        "\n",
        "guide = autoguide.AutoDiagonalNormal(bcnn, init_scale=inital_variance)\n",
        "\n",
        "\n",
        "# guide = autoguide.AutoMultivariateNormal(bcnn, init_scale=0.01) # this gives an error: parameter loc has invalid values....\n",
        "# guide = autoguide.AutoLowRankMultivariateNormal(bcnn)\n",
        "bcnn = bcnn.to(device)\n",
        "guide = guide.to(device)\n",
        "\n",
        "# l = guide(torch.rand(1, *input_dim),1)\n",
        "# print(f'Sum of Guide Variances BEFORE: {pyro.param(\"AutoDiagonalNormal.scale\").sum()}')\n",
        "# print(f'Sum of Guide Mean BEFORE: {pyro.param(\"AutoDiagonalNormal.loc\").abs().sum()}')\n",
        "\n",
        "\n",
        "# bcnn(images)\n",
        "# ipdb.set_trace()\n",
        "# svi = SVI(model = bcnn, guide = guide, optim=pyro_ka_Adam({'lr':0.001}), loss=Trace_ELBO())\n",
        "# Reminder: dont mix positional and named argument to pyro functions....weird things happen.\n",
        "\n",
        "from pyro.optim import ExponentialLR \n",
        "momentum = 0.1\n",
        "num_steps = 10000\n",
        "gamma = 0.1\n",
        "lrd = gamma**(1/num_steps)\n",
        "\n",
        "schedulerELR = ExponentialLR({'optimizer': torch.optim.Adam, 'optim_args': {'lr': learning_rate}, 'gamma': 0.1})\n",
        "svi = SVI(model = bcnn, guide = guide,\n",
        "          # optim=schedulerELR,\n",
        "          # optim=pyro.optim.SGD({\"lr\": learning_rate, \"momentum\":momentum}),\n",
        "          #  optim = pyro.optim.SGD({\"lr\":learning_rate}),\n",
        "           optim = pyro.optim.Adam({\"lr\":learning_rate, 'betas' :(0.95, 0.999)}),\n",
        "          #  optim = pyro.optim.ClippedAdam({\"lr\":learning_rate, 'lrd':lrd, 'betas' :(0.95, 0.999)}),\n",
        "          #  optim = pyro.optim.ClippedAdam({\"lr\":learning_rate, 'betas' :(0.95, 0.999)}),\n",
        "          loss=Trace_ELBO()\n",
        "          # loss=Trace_ELBO(num_particles=10,vectorize_particles=True)\n",
        "          # loss=Trace_ELBO(num_particles=10,vectorize_particles=False)\n",
        "\n",
        "          )\n",
        "\n",
        "\n",
        "# for n in pyro.get_param_store():#guide.named_parameters():\n",
        "#   print(n)\n",
        "\n",
        "\n",
        "def early_stopping(loss_list, tol=0.01, lookback=5):\n",
        "  if(len(loss_list)<lookback+1):\n",
        "    return (False)\n",
        "  per_change =  np.array([np.abs(x/y-1) for (x,y) in zip(loss_list[-lookback:], loss_list[-(lookback+1):-1])])\n",
        "  return(np.all(per_change<tol))\n",
        "\n",
        "\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# latent_dim = 1790 # guide.latent_dim\n",
        "# pyro.param(\"auto_loc\", torch.randn(latent_dim))\n",
        "# pyro.param(\"auto_scale\", torch.ones(latent_dim),\n",
        "#       constraint=constraints.positive)\n",
        "# l = guide(image, labels)\n",
        "\n",
        "# l = guide(torch.rand(1, *input_dim),1)\n",
        "# print(f'Sum of Guide Variances BEFORE: {pyro.param(\"AutoDiagonalNormal.scale\").sum()}')\n",
        "# print(f'Sum of Guide Mean BEFORE: {pyro.param(\"AutoDiagonalNormal.loc\").abs().sum()}')\n",
        "\n",
        "start_training_time = time.time()\n",
        "ELBO_loss = []\n",
        "\n",
        "for i in range(n_epochs):\n",
        "  running_loss = 0\n",
        "  batch = 0\n",
        "  epoch_loss=0\n",
        "  for images, labels in train_loader:\n",
        "    batch+=1\n",
        "    # print(f'epoch: {i}  Batch: {batch}')\n",
        "    images = images.to(device)\n",
        "    labels = labels.float().to(device)\n",
        "    # ipdb.set_trace() \n",
        "    elbo = svi.step(images, labels.squeeze())\n",
        "    epoch_loss += elbo\n",
        "    # logging.info(\"Elbo loss: {}\".format(elbo))\n",
        "    # if batch%50==0:\n",
        "    #       print(f'epoch: {i} batch:{batch}  ELBO LOSS: {elbo} ')\n",
        "    # print(f'------------------------------------- {i}')\n",
        "    # break\n",
        "  ELBO_loss.append(epoch_loss)\n",
        "  lookback=5\n",
        "  loss_list=ELBO_loss.copy()\n",
        "  if(len(loss_list)>lookback+1):\n",
        "    per_change =  np.array([np.abs(x/y-1) for (x,y) in zip(loss_list[-lookback:], loss_list[-(lookback+1):-1])])\n",
        "    print(per_change)\n",
        "  print(f'epoch: {i},  loss:{epoch_loss},  Guide VAriance: {pyro.param(\"AutoDiagonalNormal.scale\").sum()}')\n",
        "  if(early_stopping(ELBO_loss, early_stopping_tol)):\n",
        "    print(f'EARLY stopping Triggered at {i}')\n",
        "    break\n",
        " \n",
        "\n",
        "# next(PlainCNNModel.parameters()).device\n",
        "# for m in PlainCNNModel.modules():\n",
        "#   for name, value in list(m.named_parameters(recurse=False)):\n",
        "#     print(name)\n",
        "\n",
        "\n",
        "# for n in pyro.get_param_store():#guide.named_parameters():\n",
        "#   print(n)\n",
        "plt.plot(ELBO_loss)\n",
        "\n",
        "print(f'Sum of Guide Variances AFTER: {pyro.param(\"AutoDiagonalNormal.scale\").sum()}')\n",
        "print(f'Sum of Guide Mean AFTER: {pyro.param(\"AutoDiagonalNormal.loc\").abs().sum()}')\n",
        "\n",
        "# gg = pyro.param(\"AutoDiagonalNormal.scale\").shape.numel()* inital_variance\n",
        "\n",
        "end_training_time = time.time()\n",
        "print(f'Training Time Taken: {(end_training_time - start_training_time)/60 }')\n",
        "print(datetime.now(tz = pytz.timezone( 'Asia/Kolkata' )))\n",
        "\n",
        "list(guide.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2027c897a384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mguide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoguide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoDiagonalNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minital_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParamStoreDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyro' has no attribute 'ParamStoreDict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lnbvm341Zr0"
      },
      "source": [
        "# guide = autoguide.AutoDiagonalNormal(bcnn, init_scale=inital_variance)\n",
        "# l = guide(torch.rand(1, *input_dim),1)\n",
        "\n",
        "# pyro.params.param_store.ParamStoreDict.named_parameters(pyro.param)\n",
        "\n",
        "# for k in pyro.get_param_store():\n",
        "#   # print(k)\n",
        "#   print(pyro.params.user_param_name(k))\n",
        "\n",
        "# get_all_param_names()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOdx4YVg3Pt"
      },
      "source": [
        "# TODO: how to use the stuff coming out of predictive.\n",
        "# see what paras chopra did:\n",
        "# https://github.com/paraschopra/bayesian-neural-network-mnist/blob/master/bnn.ipynb\n",
        "# https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
        "# https://colab.research.google.com/drive/1pdYb15kdJLz8GkWNbc0mhb4GuEvDhgsn#scrollTo=5N0Ama80QkJ2\n",
        "from datetime import datetime, timezone\n",
        "import pytz\n",
        "print(datetime.now(tz = pytz.timezone( 'Asia/Kolkata' )))\n",
        "\n",
        "# Predict:\n",
        "from pyro.infer import Predictive\n",
        "\n",
        "start_prediction_time = time.time()\n",
        "\n",
        "preds= Predictive(model = bcnn, guide = guide, num_samples = n_samples, return_sites=(\"_RETURN\", \"obs\"))\n",
        "correct_preds = 0\n",
        "\n",
        "num_batches_to_run_for = 10\n",
        "batch_num = 0\n",
        "predictions_all = []\n",
        "for img, labels in test_loader:\n",
        "  samples = preds(img)\n",
        "  predicted = np.argmax(samples['_RETURN'].mean(axis=[0]), axis=1)\n",
        "  predictions_all.extend(predicted.tolist())\n",
        "  correct_preds += (predicted == labels).sum()\n",
        "  # if(num_batches_to_run_for == batch_num):\n",
        "  #   break;\n",
        "  batch_num +=1\n",
        "\n",
        "# test_accuracy = correct_preds/ ((num_batches_to_run_for+1)*test_loader.batch_size) #len(test_loader.dataset)\n",
        "test_accuracy = correct_preds/len(predictions_all)\n",
        "print(\"---------------------------------\")\n",
        "print(f'test accuracy:{test_accuracy}')\n",
        "end_prediction_time = time.time()\n",
        "print(f'Prediction Time Taken: {start_prediction_time - end_prediction_time}')\n",
        "\n",
        "samples.keys()\n",
        "print(labels)\n",
        "# print(samples['obs'])\n",
        "# print(samples['_RETURN'])\n",
        "# print(samples['_RETURN'].shape)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  # plt.imshow(img[i].squeeze())\n",
        "  # plt.show()\n",
        "  print(f'Actual label:{labels[i]}   Predictions: {samples[\"obs\"][:,i]}')\n",
        "\n",
        "# ADAM\n",
        "# batSize SampleSize accuracy, epochs, variance\n",
        "# 8, 200, 0.125 , 10epochs, 1\n",
        "# 8, 1000, 0.10 , 10epochs, 1\n",
        "# 32, 200, 0.15 , 10epochs, 1\n",
        "# 32, 1000 0.16 , 10epochs, 1\n",
        "# 64, 200  .13 , 10epochs, 1\n",
        "# 64, 1000 .13 , 10epochs, 1\n",
        "# 128, 200 0.86 , 10epochs, 1\n",
        "#128, 200, 87, 40 epochs, 0.0001\n",
        "#128, 200, 84, 80 epochs, 0.001\n",
        "#128, 200, 83, 50 epochs, 0.01\n",
        "#128, 200, 80, 30 epochs, 0.1\n",
        "\n",
        "# 128, 1000 0.8656 , 10epochs, 1\n",
        "# 1048 1000 78 , 10epochs, 1\n",
        "# 1048 200 71 , 10epochs, 1\n",
        "# 1048 200 83  20 epochs, 1\n",
        "# 1048 200 90  40 epochs, 1\n",
        "# 1048 200 90  80 epochs, 1\n",
        "# 1048 1000 90  80 epochs, AutoguideVar=0.1\n",
        "# 1048 2000 71, 10epochs, AutoguideVar=0.1\n",
        "# 512 200 92, 40epochs, AutoguideVar=0.1\n",
        "#10000, 200, 34, 40 epochs, AutoguideVar=0.1\n",
        "#10000, 200, 78, 80 epochs, AutoguideVar=0.1\n",
        "\n",
        "#5000, 200, 75, 40 epochs, AutoguideVar=0.1\n",
        "#5000, 200, 76, 80 epochs, AutoguideVar=0.1\n",
        "#5000, 200, 92, 80 epochs, AutoguideVar=0.05\n",
        "#5000, 200, 95, 80 epochs, AutoguideVar=0.01\n",
        "#5000, 200, 94, 80 epochs, AutoguideVar=0.001\n",
        "#5000, 200, 96.6, 100 epochs, AutoguideVar=0.001\n",
        "#5000, 200, 93, 100 epochs, AutoguideVar=0.1\n",
        "\n",
        "#5000, 200, 97, 100 epochs, AutoguideVar=0.001\n",
        "#5000, 100, 97, 100 epochs, AutoguideVar=0.001\n",
        "#5000, 100, 97.5, 120 epochs, AutoguideVar=0.001\n",
        "#5000, 100, 97.7, 100 epochs, AutoguideVar=0.0001\n",
        "\n",
        "#50K, 200, 33, 55 epochs, AutoguideVar=0.001\n",
        "#50K, 200, 52, 80 epochs, AutoguideVar=0.001\n",
        "#50K, 200, 59, 120 epochs, AutoguideVar=0.001\n",
        "#50K, 200, 58, 160 epochs, AutoguideVar=0.001\n",
        "#50K, 1000, 59, 160 epochs, AutoguideVar=0.001\n",
        "\n",
        "# running a jagged elo curve wala with particle thing to se if its smoother. Observation - the particle one was also jagged only."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARXuAcOZ0YP1"
      },
      "source": [
        "# samples = preds(img)\n",
        "# predicted = np.argmax(samples['_RETURN'].mean(axis=[0]), axis=1)\n",
        "sm = nn.LogSoftmax(dim=-1)\n",
        "val = sm(samples['_RETURN'])\n",
        "val.shape\n",
        "\n",
        "np.exp(val[0,0,:]).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yw8vK0dxD5c"
      },
      "source": [
        "from matplotlib import colors\n",
        "\n",
        "\n",
        "classes = ('0', '1', '2', '3','4', '5', '6', '7', '8', '9')\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    #plt.imshow(npimg,  cmap='gray')\n",
        "    #fig.show(figsize=(1,1))    \n",
        "    fig, ax = plt.subplots(figsize=(1, 1))\n",
        "    # ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n",
        "    plt.imshow(npimg,  cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "def test_batch(preds, images, labels, plot=True):\n",
        "    lsm = nn.LogSoftmax(dim=-1)\n",
        "    sm = nn.Softmax(dim=-1)\n",
        "    y = (lsm(preds(images)['_RETURN']))\n",
        "    # y = lsm(lsm(preds(images)['_RETURN'])) ## double log soft max to coincide with what paras chopra has done. Doesnt seem to work.\n",
        "\n",
        "    # y = (preds(images)['_RETURN']) #logits\n",
        "    # prob_y = lsm(preds(images)['_RETURN']) # log_probabilities\n",
        "\n",
        "    # ipdb.set_trace()\n",
        "    predicted_for_images = 0\n",
        "    correct_predictions=0\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "    \n",
        "        if(plot):\n",
        "            print(\"-----------------------------------------------------------------------------------------------------\")\n",
        "            print(\"Real: \",labels[i].item())\n",
        "            # fig, axs = plt.subplots(1, 10, sharex=True, sharey=True, figsize=(20,2))\n",
        "            fig, axs = plt.subplots(1, 10, sharey=True, figsize=(20,2))\n",
        "            \n",
        "    \n",
        "        all_digits_prob = []\n",
        "    \n",
        "        highted_something = False\n",
        "    \n",
        "        for j in range(len(classes)):\n",
        "        \n",
        "            highlight=False\n",
        "        \n",
        "            histo = []\n",
        "            histo_exp = []\n",
        "        \n",
        "            for z in range(y.shape[0]):\n",
        "                histo.append(y[z][i][j])\n",
        "                histo_exp.append(np.exp(y[z][i][j]))\n",
        "                # histo_exp.append(np.exp(prob_y[z][i][j]))\n",
        "                \n",
        "            \n",
        "            prob = np.percentile(histo_exp, 50) #sampling median probability\n",
        "        \n",
        "            if(prob>0.2): #select if network thinks this sample is 20% chance of this being a label\n",
        "                highlight = True #possibly an answer\n",
        "        \n",
        "            all_digits_prob.append(prob)\n",
        "            \n",
        "            if(plot):\n",
        "            \n",
        "                N, bins, patches = axs[j].hist(histo, bins=8, color = \"lightgray\", lw=0,  weights=np.ones(len(histo)) / len(histo), density=False)\n",
        "                # N, bins, patches = axs[j].hist(histo, bins=(np.array([0, 1.,2.,3.,4.,5.,6.,7.,8.,9.,10.])*1/10.).tolist()    , color = \"lightgray\", lw=0,  weights=np.ones(len(histo)) / len(histo), density=False)\n",
        "                # N, bins, patches = axs[j].hist(histo_exp, bins=10, color = None,   rwidth=100,align='mid', #lw=0,\n",
        "                #                                weights=np.ones(len(histo)) / len(histo), density=False)\n",
        "                # N, bins, patches = axs[j].hist(histo_exp, bins=2, color = \"lightgray\", histtype = 'stepfilled', lw=0,  weights=np.ones(len(histo)) / len(histo), density=True)\n",
        "                axs[j].set_title(str(j)+\" (\"+str(round(prob,2))+\")\") \n",
        "        \n",
        "            if(highlight):\n",
        "            \n",
        "                highted_something = True\n",
        "                \n",
        "                if(plot):\n",
        "\n",
        "                    # We'll color code by height, but you could use any scalar\n",
        "                    fracs = N / N.max()\n",
        "\n",
        "                    # we need to normalize the data to 0..1 for the full range of the colormap\n",
        "                    norm = colors.Normalize(fracs.min(), fracs.max())\n",
        "\n",
        "                    # Now, we'll loop through our objects and set the color of each accordingly\n",
        "                    for thisfrac, thispatch in zip(fracs, patches):\n",
        "                        color = plt.cm.viridis(norm(thisfrac))\n",
        "                        thispatch.set_facecolor(color)\n",
        "\n",
        "    \n",
        "        if(plot):\n",
        "            plt.show()\n",
        "    \n",
        "        predicted = np.argmax(all_digits_prob)\n",
        "    \n",
        "        if(highted_something):\n",
        "            predicted_for_images+=1\n",
        "            if(labels[i].item()==predicted):\n",
        "                if(plot):\n",
        "                    print(\"Correct\")\n",
        "                correct_predictions +=1.0\n",
        "            else:\n",
        "                if(plot):\n",
        "                    print(f\"Incorrectly predicted as:{predicted}\")\n",
        "        else:\n",
        "            if(plot):\n",
        "                print(\"Undecided.\")\n",
        "        \n",
        "        if(plot):\n",
        "            imshow(images[i].squeeze())\n",
        "        \n",
        "    \n",
        "    if(plot):\n",
        "        print(\"Summary\")\n",
        "        print(\"Total images: \",len(labels))\n",
        "        print(\"Predicted for: \",predicted_for_images)\n",
        "        print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
        "        \n",
        "    return len(labels), correct_predictions, predicted_for_images \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSwrlpSxD8y"
      },
      "source": [
        "preds= Predictive(model = bcnn, guide = guide, num_samples = 100, return_sites=(\"_RETURN\", \"obs\"))\n",
        "\n",
        "for img, labels in test_loader:\n",
        "  # samples = preds(img)\n",
        "  break;\n",
        "n_images = 20\n",
        "test_image=img[:n_images]\n",
        "test_label = labels[:n_images].numpy()\n",
        "_, _, _ = test_batch(preds=preds, images=test_image, labels=test_label, plot=True)\n",
        "\n",
        "  # predicted = np.argmax(samples['_RETURN'].mean(axis=[0]), axis=1)\n",
        "  # predictions_all.extend(predicted.tolist())\n",
        "  # correct_preds += (predicted == labels).sum()\n",
        "  # if(num_batches_to_run_for == batch_num):\n",
        "  #   break;\n",
        "  # batch_num +=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prediction when network can decide not to predict\n",
        "\n",
        "# print('Prediction when network can refuse')\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# total_predicted_for = 0\n",
        "\n",
        "# for img, labels in test_loader:\n",
        "#     # images, labels = data\n",
        "#     total_minibatch, correct_minibatch, predictions_minibatch = test_batch(preds, images, labels, plot=False)\n",
        "#     total += total_minibatch\n",
        "#     correct += correct_minibatch\n",
        "#     total_predicted_for += predictions_minibatch\n",
        "\n",
        "# print(\"Total images: \", total)\n",
        "# print(\"Skipped: \", total-total_predicted_for)\n",
        "# print(\"Accuracy when made predictions: %d %%\" % (100 * correct / total_predicted_for))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIAjgmNdclhG"
      },
      "source": [
        "# Random Noise as inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0vaTUhEcjpD"
      },
      "source": [
        "preds= Predictive(model = bcnn, guide = guide, num_samples = 100, return_sites=(\"_RETURN\", \"obs\"))\n",
        "images_random = torch.rand(10,1,28,28)\n",
        "labels_random = torch.randint(0,10, (10,))\n",
        "test_batch(preds=preds, images=images_random, labels=labels_random, plot=True)\n",
        "\n",
        "# logits = torch.tensor([-10.,-2.,-3.,-4.])\n",
        "# smm = nn.Softmax(dim=-1)\n",
        "# smm(logits).sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROimO9m-cjso"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uExbzmvfRBAd"
      },
      "source": [
        "#Appendix/Trash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZcW6h5Aga7U"
      },
      "source": [
        "# lets see if the parameters of the guide change.\n",
        "# guide.requires_grad_(False)\n",
        "\n",
        "for name, value in pyro.get_param_store().items():\n",
        "    print(name)\n",
        "    print(name, pyro.param(name))\n",
        "    print(pyro.param(name).shape)\n",
        "sum(p.numel() for p in guide.parameters())\n",
        "8466*2\n",
        "\n",
        "pyro.param(\"AutoDiagonalNormal.scale\").sum()\n",
        "\n",
        "\n",
        "# AutoDiagonalNormal.loc Parameter containing:\n",
        "# tensor([ 1.1677, -0.0818,  0.0145,  ...,  0.0103, -0.0278, -0.1930],\n",
        "#        requires_grad=True)\n",
        "# torch.Size([8466])\n",
        "# AutoDiagonalNormal.scale tensor([0.1279, 0.1280, 0.1281,  ..., 0.1281, 0.1281, 0.1280],\n",
        "#        grad_fn=<AddBackward0>)\n",
        "# torch.Size([8466])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUndR4Zlg3TI"
      },
      "source": [
        "print('hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7JtQkEVg3Wh"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}